"""
Indexing module for adding test cases and requirements to vector store.

Handles batch indexing, progress tracking, and format conversion.
"""

from typing import List, Dict, Any, Optional
from tqdm import tqdm
import logging

from .vector_store import VectorStore
from .embeddings import EmbeddingGenerator

logger = logging.getLogger(__name__)


class VectorIndexer:
    """Index test cases and requirements into vector store."""
    
    def __init__(
        self,
        vector_store: VectorStore,
        embedding_generator: EmbeddingGenerator,
    ):
        """
        Initialize vector indexer.
        
        Args:
            vector_store: Vector store instance
            embedding_generator: Embedding generator instance
        """
        self.vector_store = vector_store
        self.embedding_generator = embedding_generator
        self.indexed_count = 0
        
        logger.info("VectorIndexer initialized")
    
    def index_test_cases(
        self,
        test_cases: List[Dict[str, Any]],
        batch_size: int = 100,
        show_progress: bool = True,
    ):
        """
        Index test cases into vector store.
        
        Args:
            test_cases: List of test case dictionaries
            batch_size: Batch size for indexing
            show_progress: Show progress bar
        """
        if not test_cases:
            logger.warning("No test cases to index")
            return
        
        logger.info(f"Indexing {len(test_cases)} test cases...")
        
        total_indexed = 0
        
        iterator = range(0, len(test_cases), batch_size)
        if show_progress:
            iterator = tqdm(iterator, desc="Indexing test cases")
        
        for i in iterator:
            batch = test_cases[i:i + batch_size]
            
            try:
                embeddings = self.embedding_generator.encode_batch(
                    batch,
                    item_type="test_case",
                    batch_size=batch_size,
                )
                
                ids = [
                    tc.get('id', f"test_case_{i + idx}")
                    for idx, tc in enumerate(batch)
                ]
                
                self.vector_store.add_documents(
                    documents=batch,
                    embeddings=embeddings,
                    ids=ids,
                )
                
                total_indexed += len(batch)
                
            except Exception as e:
                logger.error(f"Failed to index batch at position {i}: {e}")
                continue
        
        self.indexed_count += total_indexed
        
        logger.info(f"Test case indexing complete: {total_indexed} indexed")
    
    def index_requirements(
        self,
        requirements: List[Dict[str, Any]],
        batch_size: int = 100,
        show_progress: bool = True,
    ):
        """
        Index requirements into vector store.
        
        Args:
            requirements: List of requirement dictionaries
            batch_size: Batch size for indexing
            show_progress: Show progress bar
        """
        if not requirements:
            logger.warning("No requirements to index")
            return
        
        logger.info(f"Indexing {len(requirements)} requirements...")
        
        total_indexed = 0
        
        iterator = range(0, len(requirements), batch_size)
        if show_progress:
            iterator = tqdm(iterator, desc="Indexing requirements")
        
        for i in iterator:
            batch = requirements[i:i + batch_size]
            
            try:
                embeddings = self.embedding_generator.encode_batch(
                    batch,
                    item_type="requirement",
                    batch_size=batch_size,
                )
                
                ids = [
                    req.get('id', f"requirement_{i + idx}")
                    for idx, req in enumerate(batch)
                ]
                
                self.vector_store.add_documents(
                    documents=batch,
                    embeddings=embeddings,
                    ids=ids,
                )
                
                total_indexed += len(batch)
                
            except Exception as e:
                logger.error(f"Failed to index requirement batch at {i}: {e}")
                continue
        
        self.indexed_count += total_indexed
        
        logger.info(f"Requirement indexing complete: {total_indexed} indexed")
    
    def index_from_generated_results(
        self,
        test_cases: List[Any],
        source: str = "hrm_generated",
        batch_size: int = 100,
    ):
        """
        Index test cases generated by HRM model.
        
        Args:
            test_cases: List of TestCase objects (from requirements_parser.schemas)
            source: Source identifier for tracking
            batch_size: Batch size for indexing
        """
        if not test_cases:
            logger.warning("No generated test cases to index")
            return
        
        logger.info(f"Indexing {len(test_cases)} HRM-generated test cases...")
        
        test_dicts = []
        for tc in test_cases:
            try:
                test_dict = {
                    "id": tc.id,
                    "description": tc.description,
                    "type": tc.type.value if hasattr(tc.type, 'value') else str(tc.type),
                    "priority": tc.priority.value if hasattr(tc.priority, 'value') else str(tc.priority),
                    "labels": tc.labels if isinstance(tc.labels, list) else [],
                    "source": source,
                    "preconditions": tc.preconditions if hasattr(tc, 'preconditions') else [],
                    "test_steps": [
                        {"step": s.step_number, "action": s.action}
                        for s in tc.test_steps
                    ] if hasattr(tc, 'test_steps') else [],
                    "expected_results": [
                        r.result for r in tc.expected_results
                    ] if hasattr(tc, 'expected_results') else [],
                }
                
                if hasattr(tc, 'source_story_id') and tc.source_story_id:
                    test_dict["source_story_id"] = tc.source_story_id
                
                if hasattr(tc, 'test_data') and tc.test_data:
                    test_dict["test_data"] = tc.test_data
                
                test_dicts.append(test_dict)
                
            except Exception as e:
                logger.warning(f"Failed to convert TestCase {getattr(tc, 'id', '?')}: {e}")
                continue
        
        if test_dicts:
            self.index_test_cases(test_dicts, batch_size=batch_size)
        else:
            logger.warning("No test cases could be converted for indexing")
    
    def index_from_jsonl(
        self,
        file_path: str,
        item_type: str = "test_case",
        batch_size: int = 100,
    ):
        """
        Index from JSONL file.
        
        Args:
            file_path: Path to JSONL file
            item_type: "test_case" or "requirement"
            batch_size: Batch size for indexing
        """
        import json
        from pathlib import Path
        
        file_path = Path(file_path)
        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return
        
        items = []
        with open(file_path, 'r') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    item = json.loads(line.strip())
                    items.append(item)
                except json.JSONDecodeError as e:
                    logger.warning(f"Failed to parse line {line_num}: {e}")
                    continue
        
        logger.info(f"Loaded {len(items)} items from {file_path}")
        
        if item_type == "test_case":
            self.index_test_cases(items, batch_size=batch_size)
        elif item_type == "requirement":
            self.index_requirements(items, batch_size=batch_size)
        else:
            raise ValueError(f"Unknown item_type: {item_type}")
    
    def delete_by_source(self, source: str):
        """
        Delete all documents from a specific source.
        
        Args:
            source: Source identifier
        """
        logger.warning(f"Delete by source not fully implemented for all backends: {source}")
    
    def get_indexing_stats(self) -> Dict[str, Any]:
        """
        Get indexing statistics.
        
        Returns:
            Statistics dictionary
        """
        vector_stats = self.vector_store.get_stats()
        
        stats = {
            "total_indexed_this_session": self.indexed_count,
            "vector_store_stats": vector_stats,
        }
        
        return stats
    
    def clear_collection(self):
        """
        Clear the entire collection (use with caution).
        """
        logger.warning("Clear collection not implemented - requires backend-specific logic")
        raise NotImplementedError("Clear collection must be implemented per backend")
