"""
Pydantic schemas for structured requirements and test case generation.

Defines data models for Epic, User Story, Acceptance Criteria, and Test Cases
following the structure from the requirements-to-test-cases specification.
"""

from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Any
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class TestType(str, Enum):
    """Test case type classification."""
    
    POSITIVE = "positive"
    NEGATIVE = "negative"
    EDGE = "edge"
    PERFORMANCE = "performance"
    SECURITY = "security"


class Priority(str, Enum):
    """Test case priority levels."""
    
    P1 = "P1"  # Critical: security, data integrity, core functionality
    P2 = "P2"  # Important: standard features, integrations
    P3 = "P3"  # Nice-to-have: edge cases, performance, UX


class AcceptanceCriteria(BaseModel):
    """Single acceptance criterion for a user story."""
    
    criteria: str = Field(..., min_length=1, description="Acceptance criterion description")
    
    class Config:
        """Pydantic config."""
        frozen = False


class UserStory(BaseModel):
    """User story with acceptance criteria and metadata."""
    
    id: str = Field(..., min_length=1, description="Unique story identifier")
    summary: str = Field(..., min_length=1, description="Story summary")
    description: str = Field(..., min_length=1, description="Detailed description")
    acceptance_criteria: List[AcceptanceCriteria] = Field(
        default_factory=list,
        description="List of acceptance criteria"
    )
    tech_stack: Optional[List[str]] = Field(
        default_factory=list,
        description="Technology stack involved"
    )
    
    @validator('acceptance_criteria')
    def validate_criteria(cls, v):
        """Ensure at least one acceptance criterion exists."""
        if not v:
            logger.warning("User story has no acceptance criteria")
        return v
    
    class Config:
        """Pydantic config."""
        frozen = False


class Epic(BaseModel):
    """Epic containing multiple user stories."""
    
    epic_id: str = Field(..., min_length=1, description="Unique epic identifier")
    title: str = Field(..., min_length=1, description="Epic title")
    user_stories: List[UserStory] = Field(
        ...,
        min_items=1,
        description="List of user stories"
    )
    tech_stack: Optional[List[str]] = Field(
        default_factory=list,
        description="Overall technology stack"
    )
    architecture: Optional[str] = Field(
        None,
        description="Architecture description (e.g., microservices, monolithic)"
    )
    
    @validator('user_stories')
    def validate_stories(cls, v):
        """Ensure at least one user story exists."""
        if not v:
            raise ValueError("Epic must contain at least one user story")
        return v
    
    class Config:
        """Pydantic config."""
        frozen = False


class TestContext(BaseModel):
    """
    Test context extracted from requirements for HRM processing.
    
    This represents a single testable scenario that will be sent to the HRM model.
    """
    
    story_id: str = Field(..., description="Associated user story ID")
    test_type: TestType = Field(..., description="Type of test case")
    requirement_text: str = Field(..., description="Requirement description for model input")
    acceptance_criterion: Optional[str] = Field(None, description="Specific criterion being tested")
    tech_context: List[str] = Field(default_factory=list, description="Technical context")
    
    class Config:
        """Pydantic config."""
        frozen = False


class TestStep(BaseModel):
    """Single test step with action and verification."""
    
    step_number: int = Field(..., ge=1, description="Step sequence number")
    action: str = Field(..., min_length=1, description="Action to perform")
    
    class Config:
        """Pydantic config."""
        frozen = False


class ExpectedResult(BaseModel):
    """Expected result or verification point."""
    
    result: str = Field(..., min_length=1, description="Expected outcome")
    
    class Config:
        """Pydantic config."""
        frozen = False


class TestCase(BaseModel):
    """
    Complete test case generated by HRM model.
    
    Matches the output format from the example Media Asset Management test cases.
    """
    
    id: str = Field(..., pattern=r"TC-\d{3,}", description="Test case ID (e.g., TC-001)")
    type: TestType = Field(..., description="Test case type")
    priority: Priority = Field(..., description="Test priority level")
    description: str = Field(..., min_length=1, description="Test case description")
    preconditions: List[str] = Field(default_factory=list, description="Pre-conditions")
    test_steps: List[TestStep] = Field(..., min_items=1, description="Test execution steps")
    expected_results: List[ExpectedResult] = Field(..., min_items=1, description="Expected outcomes")
    test_data: Optional[str] = Field(None, description="Test data description")
    labels: List[str] = Field(default_factory=list, description="Tags for categorization")
    source_story_id: Optional[str] = Field(None, description="Source user story ID")
    
    @validator('test_steps')
    def validate_steps(cls, v):
        """Ensure steps are properly sequenced."""
        if not v:
            raise ValueError("Test case must have at least one test step")
        expected_step = 1
        for step in v:
            if step.step_number != expected_step:
                logger.warning(f"Step numbering gap: expected {expected_step}, got {step.step_number}")
            expected_step = step.step_number + 1
        return v
    
    class Config:
        """Pydantic config."""
        frozen = False


class GenerationMetadata(BaseModel):
    """Metadata about the test case generation process."""
    
    model_checkpoint: str = Field(..., description="HRM checkpoint used")
    generation_time_seconds: float = Field(..., ge=0, description="Time taken to generate")
    num_test_cases: int = Field(..., ge=0, description="Number of test cases generated")
    coverage_score: float = Field(0.0, ge=0.0, le=1.0, description="Test coverage score")
    model_confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="Model confidence")
    
    class Config:
        """Pydantic config."""
        frozen = False


class CoverageReport(BaseModel):
    """Test coverage analysis report."""
    
    positive_tests: int = Field(0, ge=0, description="Number of positive test cases")
    negative_tests: int = Field(0, ge=0, description="Number of negative test cases")
    edge_tests: int = Field(0, ge=0, description="Number of edge test cases")
    acceptance_criteria_covered: int = Field(0, ge=0, description="Criteria with test coverage")
    total_acceptance_criteria: int = Field(0, ge=0, description="Total criteria")
    coverage_percentage: float = Field(0.0, ge=0.0, le=100.0, description="Coverage percentage")
    missing_scenarios: List[str] = Field(default_factory=list, description="Uncovered scenarios")
    
    class Config:
        """Pydantic config."""
        frozen = False


class UserFeedback(BaseModel):
    """User feedback on generated test cases for fine-tuning."""
    
    test_case_id: str = Field(..., description="Test case ID")
    rating: int = Field(..., ge=1, le=5, description="Quality rating (1-5)")
    feedback_text: Optional[str] = Field(None, description="Textual feedback")
    corrections: Optional[Dict[str, Any]] = Field(None, description="Suggested corrections")
    timestamp: Optional[str] = Field(None, description="Feedback timestamp")
    
    class Config:
        """Pydantic config."""
        frozen = False

