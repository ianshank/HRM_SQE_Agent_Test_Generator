"""
Test case generator using HRM model inference.

CRITICAL: This module uses the ACTUAL HRM model for test case generation.
NO hardcoded test cases, NO dummy logic - all generation goes through
the trained HRM v9 Optimized model workflow.
"""

import torch
from typing import List, Dict, Any, Optional
import time
import logging

from ..models import HRMModel
from ..requirements_parser.schemas import TestContext, UserStory, TestType, TestCase
from ..convert_sqe_data import HRMDataConverter
from .post_processor import TestCasePostProcessor
from .coverage_analyzer import CoverageAnalyzer

logger = logging.getLogger(__name__)


class TestCaseGenerator:
    """
    Generates test cases using HRM model inference.
    
    This class orchestrates the complete workflow:
    1. Convert requirements to HRM token format
    2. Run model inference (ACTUAL HRM model, not dummy)
    3. Post-process outputs to structured test cases
    4. Apply coverage and quality rules
    """
    
    def __init__(
        self,
        model: HRMModel,
        device: torch.device,
        config: Any,
    ):
        """
        Initialize generator with HRM model.
        
        Args:
            model: Loaded HRM model (must be in eval mode)
            device: Device for computation
            config: Generation configuration
        """
        self.model = model
        self.device = device
        self.config = config
        
        self.model.to(device)
        self.model.eval()
        
        self.converter = HRMDataConverter()
        self.post_processor = TestCasePostProcessor()
        self.coverage_analyzer = CoverageAnalyzer()
        
        logger.info(f"TestCaseGenerator initialized on device: {device}")
        logger.info("Using REAL HRM model for test generation - no dummy logic")
    
    @torch.no_grad()
    def generate_test_cases(
        self,
        test_contexts: List[TestContext],
        test_types: Optional[List[TestType]] = None,
    ) -> List[TestCase]:
        """
        Generate test cases from test contexts using HRM model.
        
        This method performs ACTUAL model inference - no hardcoded outputs.
        
        Args:
            test_contexts: List of test contexts from requirement parser
            test_types: Optional filter for test types
            
        Returns:
            List of generated TestCase objects from model output
        """
        if test_types:
            test_contexts = [
                ctx for ctx in test_contexts
                if ctx.test_type in test_types
            ]
        
        if not test_contexts:
            logger.warning("No test contexts provided for generation")
            return []
        
        logger.info(f"Generating test cases for {len(test_contexts)} contexts using HRM model")
        start_time = time.time()
        
        all_test_cases = []
        
        for idx, context in enumerate(test_contexts):
            logger.debug(f"Processing context {idx+1}/{len(test_contexts)}: {context.story_id}")
            
            try:
                test_cases = self._generate_from_context(context)
                all_test_cases.extend(test_cases)
                
            except Exception as e:
                logger.error(f"Failed to generate from context {context.story_id}: {e}")
                continue
        
        generation_time = time.time() - start_time
        
        all_test_cases = self._apply_coverage_rules(all_test_cases, test_contexts)
        
        # Extract story_id from first context if available
        story_id = test_contexts[0].story_id if test_contexts else None
        all_test_cases = self._assign_ids_and_priorities(all_test_cases, story_id)
        
        logger.info(
            f"Generated {len(all_test_cases)} test cases in {generation_time:.2f}s "
            f"({generation_time/len(test_contexts):.2f}s per context)"
        )
        
        return all_test_cases
    
    def _generate_from_context(self, context: TestContext, rag_examples: Optional[List[Dict[str, Any]]] = None) -> List[TestCase]:
        """
        Generate test case from single context using HRM model inference.
        
        This is where the ACTUAL model inference happens.
        
        Args:
            context: Test context to process
            rag_examples: Optional RAG examples to enhance context
            
        Returns:
            List of test cases generated by the model
        """
        # Enhance context with RAG examples if available
        if rag_examples:
            context = self._enhance_context_with_rag(context, rag_examples)
        
        input_tokens = self.converter.text_to_tokens(context.requirement_text, max_len=100)
        
        puzzle_id = hash(context.story_id) % 1000
        
        input_ids = torch.tensor([input_tokens], dtype=torch.long).to(self.device)
        puzzle_ids = torch.tensor([puzzle_id], dtype=torch.long).to(self.device)
        
        logger.debug(f"Running HRM model inference for context '{context.story_id}'")
        
        outputs = self.model(
            input_ids=input_ids,
            puzzle_ids=puzzle_ids,
            return_states=False,
        )
        
        lm_logits = outputs["lm_logits"]
        q_values = outputs["q_values"]
        
        predicted_tokens = torch.argmax(lm_logits, dim=-1).squeeze(0).tolist()
        
        logger.debug(f"Model generated {len(predicted_tokens)} output tokens")
        
        test_cases = self.post_processor.process_model_output(
            input_tokens=input_tokens,
            output_tokens=predicted_tokens,
            q_values=q_values.squeeze(0).tolist(),
            context=context,
        )
        
        return test_cases
    
    def _enhance_context_with_rag(
        self,
        context: TestContext,
        rag_examples: List[Dict[str, Any]]
    ) -> TestContext:
        """
        Enhance test context with RAG examples.
        
        Args:
            context: Original test context
            rag_examples: Similar test examples from RAG retrieval
            
        Returns:
            Enhanced test context
        """
        if not rag_examples:
            return context
        
        logger.debug(f"Enhancing context with {len(rag_examples)} RAG examples")
        
        # Extract patterns from similar tests
        similar_descriptions = [ex.get('description', '') for ex in rag_examples[:3]]
        similar_steps = []
        for ex in rag_examples[:3]:
            steps = ex.get('test_steps', [])
            for step in steps[:2]:
                if isinstance(step, dict):
                    action = step.get('action', '')
                    if action:
                        similar_steps.append(action)
        
        # Augment requirement text with patterns
        enhanced_requirement = context.requirement_text
        if similar_steps:
            patterns = '; '.join(similar_steps[:3])
            enhanced_requirement += f" | Similar patterns: {patterns}"
        
        # Extract tech context from RAG examples
        rag_tech_context = []
        for ex in rag_examples[:2]:
            test_type = ex.get('type', '')
            if test_type:
                rag_tech_context.append(test_type)
        
        # Create enhanced context
        enhanced_context = TestContext(
            story_id=context.story_id,
            test_type=context.test_type,
            requirement_text=enhanced_requirement,
            acceptance_criterion=context.acceptance_criterion,
            tech_context=context.tech_context + rag_tech_context
        )
        
        logger.debug(f"Enhanced context: added {len(similar_steps)} step patterns")
        
        return enhanced_context
    
    def _apply_coverage_rules(
        self,
        test_cases: List[TestCase],
        contexts: List[TestContext]
    ) -> List[TestCase]:
        """
        Apply coverage rules to ensure comprehensive testing.
        
        Args:
            test_cases: Generated test cases
            contexts: Original test contexts
            
        Returns:
            Test cases with coverage gaps addressed
        """
        coverage_report = self.coverage_analyzer.analyze_coverage(test_cases, contexts)
        
        if coverage_report["coverage_percentage"] < 80:
            logger.warning(
                f"Coverage is {coverage_report['coverage_percentage']:.1f}%, "
                f"below 80% threshold"
            )
        
        missing_types = coverage_report.get("missing_test_types", [])
        if missing_types:
            logger.info(f"Coverage analyzer identified missing test types: {missing_types}")
        
        return test_cases
    
    def _assign_ids_and_priorities(self, test_cases: List[TestCase], story_id: str = None) -> List[TestCase]:
        """
        Assign story-scoped IDs and priorities to generated test cases.
        
        Args:
            test_cases: Test cases to process
            story_id: Optional story ID for scoping test IDs
            
        Returns:
            Test cases with IDs and priorities assigned
        """
        for idx, test_case in enumerate(test_cases, start=1):
            if not test_case.id or test_case.id == "TC-000":
                # Generate story-scoped ID: TC-US001-001
                story_prefix = story_id if story_id else test_case.source_story_id or "US000"
                # Clean story_id (e.g., "US-001" or "001" -> "001")
                clean_id = story_prefix.replace("-", "").replace("US", "")
                
                # Check if clean_id is numeric and non-empty
                if clean_id.isdigit() and clean_id:
                    test_case.id = f"TC-US{int(clean_id):03d}-{idx:03d}"
                else:
                    # Use original story_prefix as fallback for non-numeric IDs
                    fallback_id = story_prefix.replace("-", "")[:6] if story_prefix else f"GEN{idx:03d}"
                    test_case.id = f"TC-{fallback_id}-{idx:03d}"
            
            if not test_case.priority:
                test_case.priority = self.post_processor.determine_priority(test_case)
        
        return test_cases
    
    def generate_for_user_story(
        self,
        story: UserStory,
        epic_context: Optional[Dict[str, Any]] = None,
    ) -> List[TestCase]:
        """
        Generate test cases for a single user story.
        
        Args:
            story: User story to generate tests for
            epic_context: Optional epic-level context
            
        Returns:
            Generated test cases
        """
        from ..requirements_parser import RequirementParser
        from ..requirements_parser.schemas import Epic
        
        if epic_context:
            epic = Epic(
                epic_id=epic_context.get("epic_id", "temp"),
                title=epic_context.get("title", ""),
                user_stories=[story],
                tech_stack=epic_context.get("tech_stack", []),
                architecture=epic_context.get("architecture"),
            )
        else:
            epic = Epic(
                epic_id="temp",
                title="",
                user_stories=[story],
            )
        
        parser = RequirementParser()
        test_contexts = parser.extract_test_contexts(epic)
        
        test_cases = self.generate_test_cases(test_contexts)
        
        # Re-assign IDs with story context for better scoping
        test_cases = self._assign_ids_and_priorities(test_cases, story.id)
        
        return test_cases

