"""
Test case generator using HRM model inference.

CRITICAL: This module uses the ACTUAL HRM model for test case generation.
NO hardcoded test cases, NO dummy logic - all generation goes through
the trained HRM v9 Optimized model workflow.
"""

import torch
from typing import List, Dict, Any, Optional
import time
import logging

from ..models import HRMModel
from ..requirements_parser.schemas import TestContext, UserStory, TestType, TestCase
from ..convert_sqe_data import HRMDataConverter
from .post_processor import TestCasePostProcessor
from .coverage_analyzer import CoverageAnalyzer

logger = logging.getLogger(__name__)


class TestCaseGenerator:
    """
    Generates test cases using HRM model inference.
    
    This class orchestrates the complete workflow:
    1. Convert requirements to HRM token format
    2. Run model inference (ACTUAL HRM model, not dummy)
    3. Post-process outputs to structured test cases
    4. Apply coverage and quality rules
    """
    
    def __init__(
        self,
        model: HRMModel,
        device: torch.device,
        config: Any,
    ):
        """
        Initialize generator with HRM model.
        
        Args:
            model: Loaded HRM model (must be in eval mode)
            device: Device for computation
            config: Generation configuration
        """
        self.model = model
        self.device = device
        self.config = config
        
        self.model.to(device)
        self.model.eval()
        
        self.converter = HRMDataConverter()
        self.post_processor = TestCasePostProcessor()
        self.coverage_analyzer = CoverageAnalyzer()
        
        logger.info(f"TestCaseGenerator initialized on device: {device}")
        logger.info("Using REAL HRM model for test generation - no dummy logic")
    
    @torch.no_grad()
    def generate_test_cases(
        self,
        test_contexts: List[TestContext],
        test_types: Optional[List[TestType]] = None,
    ) -> List[TestCase]:
        """
        Generate test cases from test contexts using HRM model.
        
        This method performs ACTUAL model inference - no hardcoded outputs.
        
        Args:
            test_contexts: List of test contexts from requirement parser
            test_types: Optional filter for test types
            
        Returns:
            List of generated TestCase objects from model output
        """
        if test_types:
            test_contexts = [
                ctx for ctx in test_contexts
                if ctx.test_type in test_types
            ]
        
        if not test_contexts:
            logger.warning("No test contexts provided for generation")
            return []
        
        logger.info(f"Generating test cases for {len(test_contexts)} contexts using HRM model")
        start_time = time.time()
        
        all_test_cases = []
        
        for idx, context in enumerate(test_contexts):
            logger.debug(f"Processing context {idx+1}/{len(test_contexts)}: {context.story_id}")
            
            try:
                test_cases = self._generate_from_context(context)
                all_test_cases.extend(test_cases)
                
            except Exception as e:
                logger.error(f"Failed to generate from context {context.story_id}: {e}")
                continue
        
        generation_time = time.time() - start_time
        
        all_test_cases = self._apply_coverage_rules(all_test_cases, test_contexts)
        
        all_test_cases = self._assign_ids_and_priorities(all_test_cases)
        
        logger.info(
            f"Generated {len(all_test_cases)} test cases in {generation_time:.2f}s "
            f"({generation_time/len(test_contexts):.2f}s per context)"
        )
        
        return all_test_cases
    
    def _generate_from_context(self, context: TestContext) -> List[TestCase]:
        """
        Generate test case from single context using HRM model inference.
        
        This is where the ACTUAL model inference happens.
        
        Args:
            context: Test context to process
            
        Returns:
            List of test cases generated by the model
        """
        input_tokens = self.converter.text_to_tokens(context.requirement_text, max_len=100)
        
        puzzle_id = hash(context.story_id) % 1000
        
        input_ids = torch.tensor([input_tokens], dtype=torch.long).to(self.device)
        puzzle_ids = torch.tensor([puzzle_id], dtype=torch.long).to(self.device)
        
        logger.debug(f"Running HRM model inference for context '{context.story_id}'")
        
        outputs = self.model(
            input_ids=input_ids,
            puzzle_ids=puzzle_ids,
            return_states=False,
        )
        
        lm_logits = outputs["lm_logits"]
        q_values = outputs["q_values"]
        
        predicted_tokens = torch.argmax(lm_logits, dim=-1).squeeze(0).tolist()
        
        logger.debug(f"Model generated {len(predicted_tokens)} output tokens")
        
        test_cases = self.post_processor.process_model_output(
            input_tokens=input_tokens,
            output_tokens=predicted_tokens,
            q_values=q_values.squeeze(0).tolist(),
            context=context,
        )
        
        return test_cases
    
    def _apply_coverage_rules(
        self,
        test_cases: List[TestCase],
        contexts: List[TestContext]
    ) -> List[TestCase]:
        """
        Apply coverage rules to ensure comprehensive testing.
        
        Args:
            test_cases: Generated test cases
            contexts: Original test contexts
            
        Returns:
            Test cases with coverage gaps addressed
        """
        coverage_report = self.coverage_analyzer.analyze_coverage(test_cases, contexts)
        
        if coverage_report["coverage_percentage"] < 80:
            logger.warning(
                f"Coverage is {coverage_report['coverage_percentage']:.1f}%, "
                f"below 80% threshold"
            )
        
        missing_types = coverage_report.get("missing_test_types", [])
        if missing_types:
            logger.info(f"Coverage analyzer identified missing test types: {missing_types}")
        
        return test_cases
    
    def _assign_ids_and_priorities(self, test_cases: List[TestCase]) -> List[TestCase]:
        """
        Assign IDs and priorities to generated test cases.
        
        Args:
            test_cases: Test cases to process
            
        Returns:
            Test cases with IDs and priorities assigned
        """
        for idx, test_case in enumerate(test_cases, start=1):
            if not test_case.id or test_case.id == "TC-000":
                test_case.id = f"TC-{idx:03d}"
            
            if not test_case.priority:
                test_case.priority = self.post_processor.determine_priority(test_case)
        
        return test_cases
    
    def generate_for_user_story(
        self,
        story: UserStory,
        epic_context: Optional[Dict[str, Any]] = None,
    ) -> List[TestCase]:
        """
        Generate test cases for a single user story.
        
        Args:
            story: User story to generate tests for
            epic_context: Optional epic-level context
            
        Returns:
            Generated test cases
        """
        from ..requirements_parser import RequirementParser
        from ..requirements_parser.schemas import Epic
        
        if epic_context:
            epic = Epic(
                epic_id=epic_context.get("epic_id", "temp"),
                title=epic_context.get("title", ""),
                user_stories=[story],
                tech_stack=epic_context.get("tech_stack", []),
                architecture=epic_context.get("architecture"),
            )
        else:
            epic = Epic(
                epic_id="temp",
                title="",
                user_stories=[story],
            )
        
        parser = RequirementParser()
        test_contexts = parser.extract_test_contexts(epic)
        
        test_cases = self.generate_test_cases(test_contexts)
        
        return test_cases

