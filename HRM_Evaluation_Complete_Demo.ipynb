{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HRM Evaluation & Test Generation System - Complete Demo\n",
        "\n",
        "## Comprehensive Tutorial and Examples\n",
        "\n",
        "This notebook provides a complete walkthrough of the HRM (Hierarchical Recurrent Model) evaluation and test generation system. You'll learn how to:\n",
        "\n",
        "- Load and inspect the HRM v9 Optimized model\n",
        "- Parse and validate requirements from epics and user stories\n",
        "- Generate comprehensive test cases using AI\n",
        "- Use the REST API for test generation\n",
        "- Integrate with multi-agent systems\n",
        "- Fine-tune the model on custom data\n",
        "- Analyze and visualize results\n",
        "\n",
        "**Author:** Ian Cruickshank  \n",
        "**Model:** HRM v9 Optimized (28M parameters)  \n",
        "**Checkpoint:** step_7566 (converged)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, ensure all dependencies are installed and the environment is configured correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install torch transformers fastapi uvicorn pydantic pyyaml matplotlib seaborn pandas\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "# Add hrm_eval to path\n",
        "sys.path.insert(0, str(Path.cwd() / 'hrm_eval'))\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Environment setup complete!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Working directory: {Path.cwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Architecture & Loading\n",
        "\n",
        "The HRM v9 Optimized model is a hierarchical dual-level transformer designed for puzzle-solving tasks, adapted for test case generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model architecture information\n",
        "with open('hrm_eval/hrm_checkpoint_eval/model_architecture.json', 'r') as f:\n",
        "    model_arch = json.load(f)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"HRM v9 Optimized Model Architecture\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total Parameters: {model_arch['summary']['total_parameters']:,}\")\n",
        "print(f\"Model Size: {model_arch['summary']['model_size_mb']:.2f} MB\")\n",
        "print(f\"Architecture: {model_arch['summary']['architecture_type']}\")\n",
        "print(f\"Number of Layers: {model_arch['summary']['num_layers']}\")\n",
        "print(\"\\nComponent Breakdown:\")\n",
        "for component, params in model_arch['component_parameters'].items():\n",
        "    print(f\"  - {component}: {params:,} parameters\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize parameter distribution\n",
        "param_data = model_arch['component_parameters']\n",
        "components = list(param_data.keys())\n",
        "params = list(param_data.values())\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(components, params, color='steelblue')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.title('HRM v9 Optimized - Parameter Distribution by Component')\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "for i, v in enumerate(params):\n",
        "    plt.text(v, i, f' {v:,}', va='center')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load checkpoint analysis\n",
        "with open('analysis/checkpoint_analysis.json', 'r') as f:\n",
        "    checkpoint_data = json.load(f)\n",
        "\n",
        "print(\"\\nCheckpoint Analysis Summary:\")\n",
        "print(f\"Best Checkpoint: {checkpoint_data['recommended_checkpoint']}\")\n",
        "print(f\"Training Status: {checkpoint_data['training_status']}\")\n",
        "print(f\"Health Status: {checkpoint_data['health_status']}\")\n",
        "print(f\"Total Training Steps: {checkpoint_data['total_steps']}\")\n",
        "print(f\"Weight Stability: {checkpoint_data['weight_change_percent']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Requirements Parsing\n",
        "\n",
        "Parse and validate requirements from epics and user stories to prepare them for test generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import requirement parsing modules\n",
        "from requirements_parser.schemas import Epic, UserStory, AcceptanceCriteria\n",
        "from requirements_parser.requirement_parser import RequirementParser\n",
        "from requirements_parser.requirement_validator import RequirementValidator\n",
        "\n",
        "# Initialize parser and validator\n",
        "parser = RequirementParser()\n",
        "validator = RequirementValidator()\n",
        "\n",
        "print(\"Requirements parsing modules loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample epic with user stories\n",
        "sample_epic = {\n",
        "    \"epic_id\": \"EPIC-001\",\n",
        "    \"title\": \"User Authentication System\",\n",
        "    \"description\": \"Implement secure user authentication with OAuth2 and JWT tokens\",\n",
        "    \"user_stories\": [\n",
        "        {\n",
        "            \"story_id\": \"US-001\",\n",
        "            \"title\": \"User Login\",\n",
        "            \"description\": \"As a user, I want to log in with email and password\",\n",
        "            \"acceptance_criteria\": [\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-001\",\n",
        "                    \"description\": \"User can enter email and password\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-002\",\n",
        "                    \"description\": \"System validates credentials against database\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-003\",\n",
        "                    \"description\": \"JWT token is generated on successful login\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-004\",\n",
        "                    \"description\": \"Login response time is less than 500ms\",\n",
        "                    \"type\": \"performance\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"story_id\": \"US-002\",\n",
        "            \"title\": \"Password Reset\",\n",
        "            \"description\": \"As a user, I want to reset my password via email\",\n",
        "            \"acceptance_criteria\": [\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-005\",\n",
        "                    \"description\": \"User can request password reset via email\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-006\",\n",
        "                    \"description\": \"Reset link expires after 24 hours\",\n",
        "                    \"type\": \"security\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Parse the epic\n",
        "epic = Epic(**sample_epic)\n",
        "print(f\"Created Epic: {epic.epic_id} - {epic.title}\")\n",
        "print(f\"User Stories: {len(epic.user_stories)}\")\n",
        "print(f\"Total Acceptance Criteria: {sum(len(us.acceptance_criteria) for us in epic.user_stories)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate the epic\n",
        "is_valid, issues = validator.validate_epic(epic)\n",
        "testability_score, testability_report = validator.check_testability(epic)\n",
        "\n",
        "print(\"\\nEpic Validation Results:\")\n",
        "print(f\"Valid: {is_valid}\")\n",
        "if issues:\n",
        "    print(\"Issues found:\")\n",
        "    for issue in issues:\n",
        "        print(f\"  - {issue}\")\n",
        "else:\n",
        "    print(\"No issues found!\")\n",
        "\n",
        "print(f\"\\nTestability Score: {testability_score:.2f}/10.0\")\n",
        "print(\"\\nTestability Report:\")\n",
        "for key, value in testability_report.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract test contexts\n",
        "test_contexts = parser.extract_test_contexts(epic)\n",
        "\n",
        "print(f\"\\nExtracted {len(test_contexts)} test contexts:\")\n",
        "for i, context in enumerate(test_contexts[:3], 1):  # Show first 3\n",
        "    print(f\"\\n{i}. {context.context_type.upper()}\")\n",
        "    print(f\"   Story: {context.user_story_id}\")\n",
        "    print(f\"   Criteria: {context.criterion_id}\")\n",
        "    print(f\"   Requirement: {context.requirement_text[:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test Case Generation\n",
        "\n",
        "Generate comprehensive test cases using the HRM model based on the parsed requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import test generation modules\n",
        "from test_generator.generator import TestCaseGenerator\n",
        "from test_generator.template_engine import TestTemplateEngine\n",
        "from test_generator.coverage_analyzer import CoverageAnalyzer\n",
        "\n",
        "print(\"Test generation modules loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize test case generator\n",
        "# Note: This requires the actual model checkpoint\n",
        "checkpoint_path = \"checkpoints_hrm_v9_optimized_step_7566\"\n",
        "\n",
        "try:\n",
        "    generator = TestCaseGenerator(\n",
        "        model_path=checkpoint_path,\n",
        "        device=device\n",
        "    )\n",
        "    print(f\"TestCaseGenerator initialized with checkpoint: {checkpoint_path}\")\n",
        "    print(f\"Using device: {device}\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Model loading requires actual checkpoint file\")\n",
        "    print(f\"For demo purposes, we'll use mock generation\")\n",
        "    generator = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate test cases (or show example structure)\n",
        "example_test_case = {\n",
        "    \"test_id\": \"TC-001\",\n",
        "    \"title\": \"Test Successful User Login with Valid Credentials\",\n",
        "    \"description\": \"Verify that a user can successfully log in with valid email and password\",\n",
        "    \"priority\": \"P1\",\n",
        "    \"test_type\": \"functional\",\n",
        "    \"preconditions\": [\n",
        "        \"User account exists in the database\",\n",
        "        \"User credentials are valid\",\n",
        "        \"Authentication service is running\"\n",
        "    ],\n",
        "    \"test_steps\": [\n",
        "        {\n",
        "            \"step_number\": 1,\n",
        "            \"action\": \"Navigate to login page\",\n",
        "            \"expected_result\": \"Login form is displayed\"\n",
        "        },\n",
        "        {\n",
        "            \"step_number\": 2,\n",
        "            \"action\": \"Enter valid email address\",\n",
        "            \"expected_result\": \"Email field accepts input\"\n",
        "        },\n",
        "        {\n",
        "            \"step_number\": 3,\n",
        "            \"action\": \"Enter valid password\",\n",
        "            \"expected_result\": \"Password field accepts masked input\"\n",
        "        },\n",
        "        {\n",
        "            \"step_number\": 4,\n",
        "            \"action\": \"Click 'Login' button\",\n",
        "            \"expected_result\": \"Authentication request is sent to server\"\n",
        "        }\n",
        "    ],\n",
        "    \"expected_outcome\": \"User is successfully authenticated and redirected to dashboard with valid JWT token\",\n",
        "    \"postconditions\": [\n",
        "        \"JWT token is stored in session\",\n",
        "        \"User session is active\",\n",
        "        \"Login timestamp is recorded\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"Example Test Case Structure:\")\n",
        "print(json.dumps(example_test_case, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format test case in different styles\n",
        "template_engine = TestTemplateEngine()\n",
        "\n",
        "# Gherkin format\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GHERKIN FORMAT (BDD Style)\")\n",
        "print(\"=\" * 60)\n",
        "gherkin_test = f\"\"\"\n",
        "Feature: User Authentication\n",
        "  As a user\n",
        "  I want to log in with my credentials\n",
        "  So that I can access my account\n",
        "\n",
        "  Scenario: {example_test_case['title']}\n",
        "    Given {example_test_case['preconditions'][0]}\n",
        "    And {example_test_case['preconditions'][1]}\n",
        "    When I navigate to the login page\n",
        "    And I enter my valid email address\n",
        "    And I enter my valid password\n",
        "    And I click the 'Login' button\n",
        "    Then {example_test_case['expected_outcome']}\n",
        "    And {example_test_case['postconditions'][0]}\n",
        "\"\"\"\n",
        "print(gherkin_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. API Service Usage\n",
        "\n",
        "The system provides a REST API for test generation. Here's how to use it programmatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example API usage (demonstrative - server would need to be running)\n",
        "import requests\n",
        "\n",
        "# API endpoint configuration\n",
        "API_BASE_URL = \"http://localhost:8000\"\n",
        "API_KEY = \"your-api-key-here\"\n",
        "\n",
        "# Example API request structure\n",
        "api_request_example = {\n",
        "    \"epic\": sample_epic,\n",
        "    \"generation_config\": {\n",
        "        \"num_test_cases\": 10,\n",
        "        \"priority_distribution\": {\n",
        "            \"P1\": 0.3,\n",
        "            \"P2\": 0.5,\n",
        "            \"P3\": 0.2\n",
        "        },\n",
        "        \"test_types\": [\"functional\", \"security\", \"performance\"],\n",
        "        \"include_edge_cases\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Example API Request:\")\n",
        "print(json.dumps(api_request_example, indent=2)[:500] + \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example API response structure\n",
        "api_response_example = {\n",
        "    \"status\": \"success\",\n",
        "    \"request_id\": \"req-12345\",\n",
        "    \"generated_test_cases\": [\n",
        "        {\n",
        "            \"test_id\": \"TC-001\",\n",
        "            \"title\": \"Test Successful User Login\",\n",
        "            \"priority\": \"P1\",\n",
        "            \"test_type\": \"functional\"\n",
        "        },\n",
        "        {\n",
        "            \"test_id\": \"TC-002\",\n",
        "            \"title\": \"Test Login with Invalid Password\",\n",
        "            \"priority\": \"P1\",\n",
        "            \"test_type\": \"security\"\n",
        "        }\n",
        "    ],\n",
        "    \"metadata\": {\n",
        "        \"num_test_cases\": 10,\n",
        "        \"generation_time_ms\": 1250,\n",
        "        \"model_version\": \"hrm_v9_optimized_step_7566\",\n",
        "        \"coverage_score\": 0.92\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nExample API Response:\")\n",
        "print(json.dumps(api_response_example, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Multi-Agent Integration\n",
        "\n",
        "Integrate the test generator with multi-agent systems for collaborative testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import agent integration modules\n",
        "from integration.agent_adapter import AgentSystemAdapter\n",
        "from integration.workflow_connector import WorkflowConnector\n",
        "\n",
        "print(\"Agent integration modules loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize agent adapter (demonstrative)\n",
        "# adapter = AgentSystemAdapter(generator, agent_id=\"test_generator_sqs\")\n",
        "\n",
        "# Agent capabilities\n",
        "agent_capabilities = [\n",
        "    \"generate_test_cases\",\n",
        "    \"analyze_coverage\",\n",
        "    \"validate_requirements\"\n",
        "]\n",
        "\n",
        "print(\"Agent Capabilities:\")\n",
        "for i, capability in enumerate(agent_capabilities, 1):\n",
        "    print(f\"{i}. {capability}\")\n",
        "\n",
        "# Example agent request\n",
        "agent_request = {\n",
        "    \"type\": \"generate_test_cases\",\n",
        "    \"request_id\": \"req-agent-001\",\n",
        "    \"requester\": \"swe_agent\",\n",
        "    \"epic\": sample_epic,\n",
        "    \"priority\": \"high\"\n",
        "}\n",
        "\n",
        "print(\"\\nExample Agent Request:\")\n",
        "print(json.dumps(agent_request, indent=2)[:400] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fine-Tuning Pipeline\n",
        "\n",
        "Continuously improve the model by fine-tuning on domain-specific data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import fine-tuning modules\n",
        "from fine_tuning.data_collector import TrainingDataCollector\n",
        "from fine_tuning.fine_tuner import HRMFineTuner\n",
        "from fine_tuning.evaluator import FineTuningEvaluator\n",
        "\n",
        "# Training data example\n",
        "training_example = {\n",
        "    \"prompt\": \"Create test cases for user login functionality with OAuth2 authentication\",\n",
        "    \"completion\": \"Test Case 1: Successful OAuth2 Login\\\\nSteps:\\\\n1. Navigate to login page\\\\n2. Click 'Login with OAuth2'\\\\n3. Enter valid credentials\\\\n4. Verify redirect to dashboard\\\\nExpected: User is authenticated with OAuth2 token\"\n",
        "}\n",
        "\n",
        "print(\"Fine-Tuning Pipeline Components:\")\n",
        "print(\"1. TrainingDataCollector - Collect feedback and examples\")\n",
        "print(\"2. HRMFineTuner - Fine-tune model on custom data\")\n",
        "print(\"3. FineTuningEvaluator - Evaluate model improvements\")\n",
        "print(\"\\nTraining Example Format:\")\n",
        "print(json.dumps(training_example, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Real-World Example: E-Commerce Testing\n",
        "\n",
        "Let's walk through a complete real-world example for an e-commerce fulfillment system.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# E-commerce fulfillment epic\n",
        "ecommerce_epic = {\n",
        "    \"epic_id\": \"EPIC-ECOM-001\",\n",
        "    \"title\": \"Order Fulfillment System\",\n",
        "    \"description\": \"Implement automated order fulfillment with inventory management\",\n",
        "    \"user_stories\": [\n",
        "        {\n",
        "            \"story_id\": \"US-ECOM-001\",\n",
        "            \"title\": \"Place Order\",\n",
        "            \"description\": \"As a customer, I want to place an order for products\",\n",
        "            \"acceptance_criteria\": [\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-ECOM-001\",\n",
        "                    \"description\": \"Customer can add products to cart\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-ECOM-002\",\n",
        "                    \"description\": \"System validates inventory availability\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-ECOM-003\",\n",
        "                    \"description\": \"Order confirmation is sent within 30 seconds\",\n",
        "                    \"type\": \"performance\"\n",
        "                }\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"story_id\": \"US-ECOM-002\",\n",
        "            \"title\": \"Track Order\",\n",
        "            \"description\": \"As a customer, I want to track my order status\",\n",
        "            \"acceptance_criteria\": [\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-ECOM-004\",\n",
        "                    \"description\": \"Customer can view real-time order status\",\n",
        "                    \"type\": \"functional\"\n",
        "                },\n",
        "                {\n",
        "                    \"criterion_id\": \"AC-ECOM-005\",\n",
        "                    \"description\": \"Tracking updates are sent via email and SMS\",\n",
        "                    \"type\": \"functional\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Parse and validate\n",
        "ecom_epic = Epic(**ecommerce_epic)\n",
        "is_valid, issues = validator.validate_epic(ecom_epic)\n",
        "score, report = validator.check_testability(ecom_epic)\n",
        "\n",
        "print(\"E-Commerce Epic:\")\n",
        "print(f\"Title: {ecom_epic.title}\")\n",
        "print(f\"User Stories: {len(ecom_epic.user_stories)}\")\n",
        "print(f\"Validation: {'PASSED' if is_valid else 'FAILED'}\")\n",
        "print(f\"Testability Score: {score:.1f}/10.0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and analyze test contexts\n",
        "ecom_contexts = parser.extract_test_contexts(ecom_epic)\n",
        "\n",
        "# Group by type\n",
        "context_types = {}\n",
        "for ctx in ecom_contexts:\n",
        "    if ctx.context_type not in context_types:\n",
        "        context_types[ctx.context_type] = 0\n",
        "    context_types[ctx.context_type] += 1\n",
        "\n",
        "print(f\"\\nExtracted {len(ecom_contexts)} test contexts:\")\n",
        "for ctx_type, count in context_types.items():\n",
        "    print(f\"  - {ctx_type.title()}: {count}\")\n",
        "\n",
        "# Visualize context distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(context_types.keys(), context_types.values(), color=['#2ecc71', '#e74c3c', '#f39c12'])\n",
        "plt.xlabel('Context Type')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Test Context Distribution for E-Commerce Epic')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "for i, (k, v) in enumerate(context_types.items()):\n",
        "    plt.text(i, v, str(v), ha='center', va='bottom')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analysis & Visualization\n",
        "\n",
        "Analyze model performance and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load detailed checkpoint analysis\n",
        "with open('analysis/detailed_checkpoint_analysis.json', 'r') as f:\n",
        "    detailed_analysis = json.load(f)\n",
        "\n",
        "# Extract training metrics\n",
        "checkpoints = detailed_analysis['checkpoints']\n",
        "steps = [cp['step'] for cp in checkpoints]\n",
        "avg_weights = [cp['statistics']['mean'] for cp in checkpoints]\n",
        "std_weights = [cp['statistics']['std'] for cp in checkpoints]\n",
        "\n",
        "# Plot weight evolution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Weight mean evolution\n",
        "ax1.plot(steps, avg_weights, marker='o', linewidth=2, markersize=6)\n",
        "ax1.set_xlabel('Training Step')\n",
        "ax1.set_ylabel('Average Weight Value')\n",
        "ax1.set_title('Model Weight Evolution During Training')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.axhline(y=avg_weights[-1], color='r', linestyle='--', alpha=0.5, label='Final Value')\n",
        "ax1.legend()\n",
        "\n",
        "# Weight std evolution\n",
        "ax2.plot(steps, std_weights, marker='s', color='orange', linewidth=2, markersize=6)\n",
        "ax2.set_xlabel('Training Step')\n",
        "ax2.set_ylabel('Weight Standard Deviation')\n",
        "ax2.set_title('Model Weight Variability During Training')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training converged at step {steps[-1]}\")\n",
        "print(f\"Final weight mean: {avg_weights[-1]:.6f}\")\n",
        "print(f\"Final weight std: {std_weights[-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a test coverage heatmap (simulated data)\n",
        "test_types = ['Functional', 'Security', 'Performance', 'Integration', 'Edge Cases']\n",
        "priorities = ['P1 (Critical)', 'P2 (Important)', 'P3 (Nice-to-have)']\n",
        "\n",
        "# Simulated coverage data\n",
        "coverage_data = np.array([\n",
        "    [0.95, 0.88, 0.92, 0.85, 0.78],  # P1\n",
        "    [0.92, 0.85, 0.88, 0.82, 0.75],  # P2\n",
        "    [0.85, 0.78, 0.80, 0.75, 0.70]   # P3\n",
        "])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(coverage_data, annot=True, fmt='.2f', cmap='RdYlGn', \n",
        "            xticklabels=test_types, yticklabels=priorities,\n",
        "            vmin=0.0, vmax=1.0, cbar_kws={'label': 'Coverage Score'})\n",
        "plt.title('Test Coverage Heatmap by Type and Priority')\n",
        "plt.xlabel('Test Type')\n",
        "plt.ylabel('Priority Level')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary & Next Steps\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Model Architecture**: HRM v9 Optimized with 28M parameters, hierarchical dual-level transformer\n",
        "2. **Requirements Parsing**: Robust validation and testability scoring\n",
        "3. **Test Generation**: AI-powered generation with NO hardcoded logic\n",
        "4. **API Integration**: Production-ready REST API with authentication\n",
        "5. **Multi-Agent Support**: Seamless integration with agent systems\n",
        "6. **Continuous Improvement**: Fine-tuning pipeline for domain adaptation\n",
        "\n",
        "### System Capabilities\n",
        "\n",
        "- Parse and validate requirements from epics and user stories\n",
        "- Generate comprehensive test cases covering multiple scenarios\n",
        "- Analyze test coverage and identify gaps\n",
        "- Format tests in various styles (Gherkin, pytest, etc.)\n",
        "- Integrate with multi-agent systems for collaborative testing\n",
        "- Fine-tune models on domain-specific data\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Deploy API**: Start the FastAPI server for production use\n",
        "2. **Fine-Tune Model**: Collect domain-specific examples and retrain\n",
        "3. **Integrate with CI/CD**: Add test generation to your pipeline\n",
        "4. **Agent Mesh**: Connect to your multi-agent system\n",
        "5. **Monitor Performance**: Track metrics and optimize\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **Documentation**: `/docs` directory\n",
        "- **API Guide**: `hrm_eval/API_USAGE_GUIDE.md`\n",
        "- **Implementation Guide**: `hrm_eval/IMPLEMENTATION_GUIDE.md`\n",
        "- **GitHub**: https://github.com/ianshank/HRM_SQE_Agent_Test_Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final statistics\n",
        "print(\"=\" * 70)\n",
        "print(\"HRM EVALUATION & TEST GENERATION SYSTEM - SESSION SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Model: HRM v9 Optimized (step_7566)\")\n",
        "print(f\"Total Parameters: 27,990,018\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Epics Processed: 2\")\n",
        "print(f\"User Stories Analyzed: {len(sample_epic['user_stories']) + len(ecommerce_epic['user_stories'])}\")\n",
        "print(f\"Test Contexts Extracted: {len(test_contexts) + len(ecom_contexts)}\")\n",
        "print(f\"Average Testability Score: {(testability_score + score) / 2:.1f}/10.0\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nThank you for using the HRM Test Generation System!\")\n",
        "print(\"For support: ianshank@gmail.com\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
